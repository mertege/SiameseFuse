{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs-555-flir-registration-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKwJSwt4ohEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Flatten,Input, Lambda, Activation, Concatenate\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "# from tensorflow.keras.preprocessing.image import image_dataset_from_directory\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EDGvcx5ya0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISmADVlGychN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grayscale_training_data = np.load('/content/drive/My Drive/grayscale_training_data.npy')  \n",
        "thermal_training_data = np.load('/content/drive/My Drive/thermal_training_data.npy') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iutAyyH6VrAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to eliminate non overlapped image pairs\n",
        "grayscale_training_data = np.delete(grayscale_training_data,np.s_[5000:8186],0)\n",
        "thermal_training_data = np.delete(thermal_training_data,np.s_[5000:8186],0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXPxh6i3-UJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grayscale_test_data = np.load('/content/drive/My Drive/grayscale_test_data.npy')  \n",
        "thermal_test_data = np.load('/content/drive/My Drive/thermal_test_data.npy') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpIIY8WI5twz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SSIM LOSS\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "lambda1 = 0\n",
        "lambda2 = 1\n",
        "lambda3 = 1\n",
        "lambda4 = 1\n",
        "def _tf_fspecial_gauss(size, sigma):\n",
        "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
        "    \"\"\"\n",
        "    x_data, y_data = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
        "\n",
        "    x_data = np.expand_dims(x_data, axis=-1)\n",
        "    x_data = np.expand_dims(x_data, axis=-1)\n",
        "\n",
        "    y_data = np.expand_dims(y_data, axis=-1)\n",
        "    y_data = np.expand_dims(y_data, axis=-1)\n",
        "\n",
        "    x = tf.constant(x_data, dtype=tf.float32)\n",
        "    y = tf.constant(y_data, dtype=tf.float32)\n",
        "\n",
        "    g = tf.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
        "    return g / tf.reduce_sum(g)\n",
        "\n",
        "\n",
        "def SSIM_(img1, img2, size=11, sigma=1.5):\n",
        "    window = _tf_fspecial_gauss(size, sigma) # window shape [size, size]\n",
        "    K1 = 0.01\n",
        "    K2 = 0.03\n",
        "    L = 1  # depth of image (255 in case the image has a differnt scale)\n",
        "    C1 = (K1*L)**2\n",
        "    C2 = (K2*L)**2\n",
        "    mu1 = tf.nn.conv2d(img1, window, strides=[1,1,1,1], padding='VALID')\n",
        "    mu2 = tf.nn.conv2d(img2, window, strides=[1,1,1,1],padding='VALID')\n",
        "    mu1_sq = mu1*mu1\n",
        "    mu2_sq = mu2*mu2\n",
        "    mu1_mu2 = mu1*mu2\n",
        "    sigma1_sq = tf.nn.conv2d(img1*img1, window, strides=[1,1,1,1],padding='VALID') - mu1_sq\n",
        "    sigma2_sq = tf.nn.conv2d(img2*img2, window, strides=[1,1,1,1],padding='VALID') - mu2_sq\n",
        "    sigma12 = tf.nn.conv2d(img1*img2, window, strides=[1,1,1,1],padding='VALID') - mu1_mu2\n",
        "\n",
        "    value = (2.0*sigma12 + C2)/(sigma1_sq + sigma2_sq + C2)\n",
        "    value = tf.reduce_mean(value)\n",
        "    return value\n",
        "\n",
        "def SSIM_LOSS(img1, img2, size=11, sigma=1.5):\n",
        "    window = _tf_fspecial_gauss(size, sigma) # window shape [size, size]\n",
        "    K1 = 0.01\n",
        "    K2 = 0.03\n",
        "    L = 1  # depth of image (255 in case the image has a differnt scale)\n",
        "    C1 = (K1*L)**2\n",
        "    C2 = (K2*L)**2\n",
        "    mu1 = tf.nn.conv2d(img1, window, strides=[1,1,1,1], padding='VALID')\n",
        "    mu2 = tf.nn.conv2d(img2, window, strides=[1,1,1,1],padding='VALID')\n",
        "    mu1_sq = mu1*mu1\n",
        "    mu2_sq = mu2*mu2\n",
        "    mu1_mu2 = mu1*mu2\n",
        "    sigma1_sq = tf.nn.conv2d(img1*img1, window, strides=[1,1,1,1],padding='VALID') - mu1_sq\n",
        "    sigma2_sq = tf.nn.conv2d(img2*img2, window, strides=[1,1,1,1],padding='VALID') - mu2_sq\n",
        "    sigma12 = tf.nn.conv2d(img1*img2, window, strides=[1,1,1,1],padding='VALID') - mu1_mu2\n",
        "\n",
        "    value = (2.0*sigma12 + C2)/(sigma1_sq + sigma2_sq + C2)\n",
        "    value = tf.reduce_mean(value)\n",
        "    return 1-value\n",
        "def correlation_coefficient_loss(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "\n",
        "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "    return 1 - K.square(r)\n",
        "def loss_funct(in1,out1):\n",
        "  return (lambda1*loss_SSIM(in1,out1) + lambda2*(tf.norm(in1-out1, ord='euclidean')) + lambda3*correlation_coefficient_loss(in1,out1) +\\\n",
        "          lambda4*loss_MS_SSIM(in1, out1))\n",
        "\n",
        "def loss_MS_SSIM(in1,out1):\n",
        "  return 1-tf.image.ssim_multiscale(in1,out1,max_val=1)\n",
        "def MS_SSIM(in1,out1):\n",
        "  return tf.image.ssim_multiscale(in1,out1,max_val=1)\n",
        "\n",
        "def loss_SSIM(in1,out1):\n",
        "  return 1-tf.image.ssim(in1,out1,max_val=1)\n",
        "def tf_SSIM(in1,out1):\n",
        "  return tf.image.ssim(in1,out1,max_val=1)\n",
        "\n",
        "def SCD(img1, fus):\n",
        "    value = corr2(fus, img1)\n",
        "    tf.cast(value, tf.float32)\n",
        "    return value\n",
        "\n",
        "def corr2(a,b):\n",
        "    a = a - K.sum(a)\n",
        "    b = b - K.sum(b)\n",
        "    r = K.sum(tf.multiply(a,b)) / K.sqrt(tf.multiply(K.sum(K.square(a)) , K.sqrt(K.sum(K.square(b)))));\n",
        "    return r\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cemfvLmy51xp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flir dataset\n",
        "def summation(vects):\n",
        "    x, y = vects\n",
        "    return x+y\n",
        "def summation_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1)\n",
        "def create_base_network(input_shape):\n",
        "    '''Base network to b3e shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = Conv2D(16, (3, 3),padding='same',kernel_initializer=\"glorot_normal\",activation='relu')(input)\n",
        "    x = Conv2D(32, (3, 3),padding='same',kernel_initializer=\"glorot_normal\",activation='relu')(x)\n",
        "    # x = Conv2D(64, (3, 3),padding='same',activation='relu')(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def decoder(input_shape):\n",
        "    input = Input(shape=input_shape)\n",
        "    # x = Conv2D(128, (3, 3),padding='same',activation='relu')(input)\n",
        "    # x = Conv2D(64, (3, 3),padding='same',activation='relu')(input)\n",
        "    x = Conv2D(32, (3, 3),padding='same',kernel_initializer=\"glorot_normal\",activation='relu')(input)\n",
        "    x = Conv2D(16, (3, 3),padding='same',kernel_initializer=\"glorot_normal\",activation='relu')(x)\n",
        "    out = Conv2D(1, (3, 3),padding='same',kernel_initializer=\"glorot_normal\",activation='relu')(x)\n",
        "    return Model(input, out)\n",
        "\n",
        "input_shape = thermal_training_data.shape[1:]\n",
        "base_network = create_base_network(input_shape)\n",
        "\n",
        "\n",
        "input_a = Input(shape=input_shape)\n",
        "input_b = Input(shape=input_shape)\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "sum_shape = thermal_training_data.shape[1:]\n",
        "aa = Lambda(summation,\n",
        "                  output_shape=sum_shape)([processed_a, processed_b])\n",
        "# aa = Concatenate()([processed_a, processed_b])\n",
        "\n",
        "base_decoder_network = decoder((256,256,32))\n",
        "out1 = base_decoder_network(aa)\n",
        "out2 = base_decoder_network(aa)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input_a, input_b], outputs=[out1,out2])       \n",
        "\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(loss=loss_funct,optimizer=opt,metrics=[loss_funct,MS_SSIM,tf_SSIM])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit([grayscale_training_data,thermal_training_data], [grayscale_training_data,thermal_training_data],\n",
        "                epochs=100,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                # validation_data=([grayscale_training_data,thermal_training_data], [grayscale_training_data,thermal_training_data]))\n",
        "                validation_data=([grayscale_test_data,thermal_test_data], [grayscale_test_data,thermal_test_data]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FyPWTUQ83FL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model\n",
        "model.save('v2_flir_lamba1_0_lamba2_1_lamba3_1_lamba4_1.h5')\n",
        "files.download('v2_flir_lamba1_0_lamba2_1_lamba3_1_lamba4_1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gq2USupCTD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aa = (history.history['val_'+model.layers[4].name+'_tf_SSIM_1'])   \n",
        "bb = (history.history['val_'+model.layers[4].name+'_tf_SSIM'])\n",
        "np.save(\"val_tf_SSIM_1\", aa)\n",
        "np.save(\"val_tf_SSIM\", bb)\n",
        "files.download('val_tf_SSIM_1.npy') \n",
        "files.download('val_tf_SSIM.npy') \n",
        "\n",
        "cc = (history.history['val_'+model.layers[4].name+'_MS_SSIM_1'])   \n",
        "dd = (history.history['val_'+model.layers[4].name+'_MS_SSIM'])\n",
        "np.save(\"val_MS_SSIM_1\", cc)\n",
        "np.save(\"val_MS_SSIM\", dd)\n",
        "files.download('val_MS_SSIM_1.npy') \n",
        "files.download('val_MS_SSIM.npy') \n",
        "\n",
        "ee = (history.history['val_'+model.layers[4].name+'_loss_funct_1'])   \n",
        "ff = (history.history['val_'+model.layers[4].name+'_loss_funct'])\n",
        "np.save(\"val_loss_funct_1\", ee)\n",
        "np.save(\"val_loss_funct\", ff)\n",
        "files.download('val_loss_funct_1.npy') \n",
        "files.download('val_loss_funct.npy') \n",
        "\n",
        "aaa = (history.history[model.layers[4].name+'_tf_SSIM_1'])   \n",
        "bbb = (history.history[model.layers[4].name+'_tf_SSIM'])\n",
        "np.save(\"_tf_SSIM_1\", aaa)\n",
        "np.save(\"_tf_SSIM\", bbb)\n",
        "files.download('_tf_SSIM_1.npy') \n",
        "files.download('_tf_SSIM.npy') \n",
        "\n",
        "ccc = (history.history[model.layers[4].name+'_MS_SSIM_1'])   \n",
        "ddd = (history.history[model.layers[4].name+'_MS_SSIM'])\n",
        "np.save(\"_MS_SSIM_1\", ccc)\n",
        "np.save(\"_MS_SSIM\", ddd)\n",
        "files.download('_MS_SSIM_1.npy') \n",
        "files.download('_MS_SSIM.npy') \n",
        "\n",
        "eee = (history.history[model.layers[4].name+'_loss_funct_1'])   \n",
        "fff = (history.history[model.layers[4].name+'_loss_funct'])\n",
        "np.save(\"_loss_funct_1\", eee)\n",
        "np.save(\"_loss_funct\", fff)\n",
        "files.download('_loss_funct_1.npy') \n",
        "files.download('_loss_funct.npy') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}